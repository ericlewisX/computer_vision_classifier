{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Traffic Signs Classifier.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMgbW+/hJcdPTomqXpYWxxw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ericlewisX/trafficsignclassifier/blob/main/Traffic_Signs_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yp5TiGYLETG0"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "except ValueError:\n",
        "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ja87rfbiEgzs"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkIgCRUkEh7D"
      },
      "source": [
        "!curl https://sdk.cloud.google.com | bash"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXrzu_bbEkW7"
      },
      "source": [
        "!gcloud init"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0LyuDDUEk24"
      },
      "source": [
        "!echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "508HGRooEnRg"
      },
      "source": [
        "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEHH62vjEo0r"
      },
      "source": [
        "!apt -qq update"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0mGniaOEqDw"
      },
      "source": [
        "!apt -qq install gcsfuse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzoY3FtXEt9S"
      },
      "source": [
        "!mkdir imagedataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUNJhoVlEvcv"
      },
      "source": [
        "## TensordBoard and Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QbWmB9OE0Q3"
      },
      "source": [
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXytVUTvE1xj"
      },
      "source": [
        "early_stopping = EarlyStopping(monitor='loss', \n",
        "                               patience=5, mode='auto'\n",
        "                               )\n",
        "model_checkpoint = ModelCheckpoint(filepath='./drive/MyDrive/Capstone3/TrafficSigns/modelX.h5', \n",
        "                                   monitor='val_accuracy', save_best_only=True)\n",
        "tensor_board = TensorBoard(log_dir=\"./logs\" )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFVsBqT8E22Y"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FW0fOxh4E7YG"
      },
      "source": [
        "## Standard\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "## Image Related\n",
        "from PIL import Image\n",
        "from skimage.io import imsave, imshow\n",
        "\n",
        "## Sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "## Tensoflow and Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import datasets, models\n",
        "from tensorflow.keras.layers import Conv2D, UpSampling2D, InputLayer, Dense, \\\n",
        "                                    Conv2DTranspose, MaxPooling2D, Dropout, Flatten, Input\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "\n",
        "path = \"imagedataset/TrafficSignClassifier/data/myData/images/\"\n",
        "label_csv = 'imagedataset/TrafficSignClassifier/labels.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3837lgNBFLH_"
      },
      "source": [
        "batch_size_val = 50  # how many to process together\n",
        "steps_per_epoch_val = 2000\n",
        "epochs_val = 3\n",
        "imageDimesions = (32,32,3)\n",
        "\n",
        "test_size = 0.2    # if 1000 images split will 200 for testing\n",
        "validation_size = 0.2 # if 1000 images 20% of remaining 800 will be 160 for validation\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uI027EP_Goh_"
      },
      "source": [
        "def load_data(path):\n",
        "  '''\n",
        "  This function loads all the images from all the directories into lists.\n",
        "  '''\n",
        "  class_directory_num, image_list, class_, shapes = 0, [], [], []\n",
        "  list_ = os.listdir(path) # Number of directories in path is synomous with number of classes\n",
        "  class_count = len(list_)\n",
        "\n",
        "  for folder in range(len(list_)):\n",
        "\n",
        "      sign_list = os.listdir(path + '/' + str(class_directory_num))\n",
        "\n",
        "      for x in sign_list:\n",
        "\n",
        "        image = cv2.imread(path + '/' + str(class_directory_num) + '/' + x)\n",
        "        image_list.append(image)\n",
        "        class_.append(class_directory_num)\n",
        "        shapes.append(image.shape)\n",
        "\n",
        "      # print(class_directory_num, end = ' ')\n",
        "      class_directory_num += 1\n",
        "\n",
        "  images_dataset_list = np.array(image_list)\n",
        "  class_num = np.array(class_)\n",
        "\n",
        "  return images_dataset_list, class_num, shapes\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNaxKz79LwZW"
      },
      "source": [
        "def save_variables(var_, file_name : str):\n",
        "  '''\n",
        "  This function saves variables as binary pickle files for reuse later. This circumvents the\n",
        "  need to rerun load_data(path) if runtime fails.\n",
        "  '''\n",
        "  with open(file_name, 'wb') as f:\n",
        "    pickle.dump(var_, f)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SHU1KhxNJJb"
      },
      "source": [
        "def load_variables(file_name : str):\n",
        "  '''\n",
        "  This function loads variables from a binary pickle file.\n",
        "  '''\n",
        "  with open(file_name, 'rb') as f:\n",
        "    var_ = pickle.load(f)\n",
        "\n",
        "  return var_\n",
        "    "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swkn0uS6NlQK"
      },
      "source": [
        "def train_test_split(images_dataset_list, class_num, test_size):\n",
        "  '''\n",
        "  This function splits our data with a train test split.  \n",
        "  '''\n",
        "  X_train, X_test, y_train, y_test = train_test_split(iamges_dataset_list, class_num, test_size = test_size)\n",
        "  \n",
        "  return X_train, X_test, y_train, y_test\n",
        " "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vacSseb1tM3X"
      },
      "source": [
        "def train_validation_split(X_train, y_train, validation_size):\n",
        "\n",
        "  '''\n",
        "  [This function splits our data with a train validation split. ]\n",
        "\n",
        "\n",
        "  \n",
        "  Returns\n",
        "  -------\n",
        "  [type] - X_train\n",
        "      [description]\n",
        "  [type] - X_validation\n",
        "      [description]\n",
        "  [type] - y_train\n",
        "      [description]\n",
        "  [type] - y_validation\n",
        "      [description]\n",
        "  '''\n",
        "\n",
        "  X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size = validation_size)\n",
        " \n",
        "  return X_train, X_validation, y_train, y_validation \n",
        " \n",
        " "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiEizxLYTQjo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}